<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding Transformer Attention · SumMi</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }

        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f7f7f8;
            --bg-tertiary: #ececec;
            --text-primary: #0d0d0d;
            --text-secondary: #6e6e80;
            --text-tertiary: #8e8ea0;
            --border-color: #e5e5e5;
            --card-bg: rgba(255, 255, 255, 0.8);
            --code-bg: #f6f8fa;
            --blur: 20px;
        }

        @media (prefers-color-scheme: dark) {
            :root {
                --bg-primary: #0d0d0d;
                --bg-secondary: #1a1a1a;
                --bg-tertiary: #2a2a2a;
                --text-primary: #ececec;
                --text-secondary: #9a9a9a;
                --text-tertiary: #6e6e6e;
                --border-color: #2a2a2a;
                --card-bg: rgba(26, 26, 26, 0.8);
                --code-bg: #1e1e1e;
            }
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.8;
            min-height: 100vh;
        }

        nav {
            position: fixed;
            top: 0; left: 0; right: 0;
            z-index: 100;
            padding: 16px 24px;
            background: var(--card-bg);
            backdrop-filter: blur(var(--blur));
            -webkit-backdrop-filter: blur(var(--blur));
            border-bottom: 1px solid var(--border-color);
        }

        .nav-content {
            max-width: 800px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .back-link {
            display: flex;
            align-items: center;
            gap: 8px;
            color: var(--text-secondary);
            text-decoration: none;
            font-size: 14px;
            font-weight: 500;
            transition: color 0.2s;
        }

        .back-link:hover { color: var(--text-primary); }
        .back-link svg { width: 16px; height: 16px; }

        .logo {
            font-size: 16px;
            font-weight: 600;
            color: var(--text-primary);
            text-decoration: none;
        }

        .article-container {
            max-width: 720px;
            margin: 0 auto;
            padding: 120px 24px 80px;
        }

        .article-header {
            margin-bottom: 48px;
            padding-bottom: 32px;
            border-bottom: 1px solid var(--border-color);
        }

        .article-meta {
            display: flex;
            align-items: center;
            gap: 16px;
            margin-bottom: 20px;
            font-size: 14px;
            color: var(--text-tertiary);
        }

        .article-tags { display: flex; gap: 8px; margin-top: 20px; }

        .tag {
            font-size: 12px;
            padding: 4px 10px;
            background: var(--bg-tertiary);
            border-radius: 4px;
            color: var(--text-secondary);
        }

        .article-title {
            font-size: 36px;
            font-weight: 600;
            line-height: 1.3;
            margin-bottom: 16px;
        }

        .article-subtitle {
            font-size: 18px;
            color: var(--text-secondary);
        }

        .article-content { font-size: 17px; line-height: 1.9; }
        .article-content h2 { font-size: 24px; font-weight: 600; margin: 48px 0 20px; }
        .article-content h3 { font-size: 20px; font-weight: 600; margin: 36px 0 16px; }
        .article-content p { margin-bottom: 20px; }
        .article-content ul, .article-content ol { margin: 20px 0; padding-left: 24px; }
        .article-content li { margin-bottom: 10px; }

        .article-content blockquote {
            margin: 28px 0;
            padding: 20px 24px;
            background: var(--bg-secondary);
            border-left: 3px solid var(--text-tertiary);
            border-radius: 0 8px 8px 0;
            color: var(--text-secondary);
            font-style: italic;
        }

        .article-content code {
            font-family: 'SF Mono', Consolas, monospace;
            font-size: 0.9em;
            padding: 2px 6px;
            background: var(--code-bg);
            border-radius: 4px;
        }

        .article-content pre {
            margin: 24px 0;
            padding: 20px;
            background: var(--code-bg);
            border-radius: 8px;
            overflow-x: auto;
        }

        .article-content pre code {
            padding: 0;
            background: none;
            font-size: 14px;
            line-height: 1.6;
        }

        .article-content hr {
            border: none;
            height: 1px;
            background: var(--border-color);
            margin: 48px 0;
        }

        .article-footer {
            margin-top: 60px;
            padding-top: 32px;
            border-top: 1px solid var(--border-color);
        }

        .author-card {
            display: flex;
            gap: 20px;
            align-items: center;
            padding: 24px;
            background: var(--bg-secondary);
            border-radius: 12px;
        }

        .author-avatar {
            width: 56px; height: 56px;
            background: linear-gradient(135deg, var(--bg-tertiary), var(--bg-secondary));
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 24px;
        }

        .author-info h4 { font-size: 16px; font-weight: 600; margin-bottom: 4px; }
        .author-info p { font-size: 14px; color: var(--text-secondary); }

        footer {
            text-align: center;
            padding: 40px 24px;
            border-top: 1px solid var(--border-color);
            margin-top: 60px;
        }

        footer p { font-size: 13px; color: var(--text-tertiary); }
        footer .secret { margin-top: 8px; font-size: 12px; opacity: 0.5; }

        @media (max-width: 768px) {
            .article-title { font-size: 28px; }
            .article-content { font-size: 16px; }
        }
    </style>
</head>
<body>

    <nav>
        <div class="nav-content">
            <a href="../index.html" class="back-link">
                <svg fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 19l-7-7 7-7"/>
                </svg>
                Back to home
            </a>
            <a href="../index.html" class="logo">SumMi</a>
        </div>
    </nav>

    <article class="article-container">
        <header class="article-header">
            <div class="article-meta">
                <span>November 25, 2024</span>
            </div>
            <h1 class="article-title">Understanding Transformer Attention</h1>
            <p class="article-subtitle">A deep dive into the self-attention mechanism that powers modern language models. From theory to implementation.</p>
            <div class="article-tags"><span class="tag">deep-learning</span><span class="tag">nlp</span><span class="tag">transformer</span></div>
        </header>

        <div class="article-content">
            <p>The Transformer architecture, introduced in the landmark paper "Attention Is All You Need" (2017), has revolutionized natural language processing and beyond. At its core lies the <strong>self-attention mechanism</strong> — a elegant solution that allows models to weigh the importance of different parts of the input when processing each element.</p>

<h2>Why Attention Matters</h2>

<p>Before Transformers, sequence models like RNNs and LSTMs processed tokens sequentially. This created two major problems:</p>

<ul><li><strong>Long-range dependencies</strong> — Information from early tokens had to pass through many steps to reach later ones, often getting diluted.</li>
<li><strong>Sequential bottleneck</strong> — Processing couldn't be parallelized, making training slow.</li>
</ul>
Attention solves both problems by allowing each token to directly attend to every other token in the sequence.

<h2>The Math Behind Self-Attention</h2>

<p>Self-attention operates on three matrices derived from the input: <strong>Query (Q)</strong>, <strong>Key (K)</strong>, and <strong>Value (V)</strong>.</p>

<pre><code class="language-">Attention(Q, K, V) = softmax(QK^T / √d_k) · V</code></pre>

<p>Let's break this down:</p>

<li><strong>QK^T</strong> — Compute similarity scores between all query-key pairs</li>
<li><strong>/ √d_k</strong> — Scale by the square root of key dimension to prevent extreme softmax values</li>
<li><strong>softmax</strong> — Normalize to get attention weights (probabilities)</li>
<li><strong>· V</strong> — Weighted sum of values based on attention weights</li>

<h2>Implementation in Python</h2>

<p>Here's a minimal implementation of scaled dot-product attention:</p>

<pre><code class="language-python">import torch
import torch.nn.functional as F

<p>def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Q, K, V: (batch, seq_len, d_k)
    """
    d_k = Q.size(-1)</p>

<p># Compute attention scores
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)</p>

<p># Apply mask (optional, for decoder)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)</p>

<p># Softmax to get attention weights
    attention_weights = F.softmax(scores, dim=-1)</p>

<p># Weighted sum of values
    output = torch.matmul(attention_weights, V)</p>

<p>return output, attention_weights</code></pre></p>

<h2>Multi-Head Attention</h2>

<p>Instead of performing a single attention function, Transformers use <strong>multi-head attention</strong> — running multiple attention operations in parallel with different learned projections.</p>

<blockquote>"Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions."<br>— Vaswani et al., 2017</blockquote>

<p>Each "head" can learn to focus on different aspects: one might capture syntactic relationships, another semantic similarities, and so on.</p>

<h2>Key Takeaways</h2>

<ul><li>Self-attention enables direct connections between all positions in a sequence</li>
<li>The Query-Key-Value framework provides a flexible way to compute relevance</li>
<li>Scaling by √d_k is crucial for stable training</li>
<li>Multi-head attention captures different types of relationships</li>
</ul>
Understanding attention is fundamental to working with modern AI systems. Whether you're fine-tuning LLMs, building RAG systems, or developing new architectures, these concepts form the foundation.
        </div>

        <footer class="article-footer">
            <div class="author-card">
                <div class="author-avatar">✦</div>
                <div class="author-info">
                    <h4>SumMi</h4>
                    <p>AI enthusiast exploring the boundaries of machine intelligence.</p>
                </div>
            </div>
        </footer>
    </article>

    <footer>
        <p>© 2024 SumMi · Exploring AI</p>
        <p class="secret">Loving you is a lonely secret.</p>
    </footer>

</body>
</html>